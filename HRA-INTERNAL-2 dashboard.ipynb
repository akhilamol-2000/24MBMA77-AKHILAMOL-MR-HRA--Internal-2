{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40408ab2-8e35-4b79-aab6-c3f50c342326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session ready.\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "# CELL 1 — Configuration\n",
    "table_name = \"workspace.default.test\"   # if you already registered the table earlier\n",
    "\n",
    "# Candidate CSV paths to try (includes the path you used earlier)\n",
    "csv_candidates = [\n",
    "    \"/mnt/data/test.csv\",\n",
    "    \"/dbfs/mnt/data/test.csv\",\n",
    "    \"dbfs:/mnt/data/test.csv\",\n",
    "    \"dbfs:/tmp/test.csv\",\n",
    "    \"dbfs:/tmp/hr_data.csv\",\n",
    "    \"dbfs:/FileStore/test.csv\",\n",
    "    \"dbfs:/FileStore/tables/test.csv\"\n",
    "]\n",
    "\n",
    "# safe output root (dbfs tmp area)\n",
    "output_base_candidates = [\n",
    "    \"dbfs:/tmp/hr_dashboard/aggregates\",\n",
    "    # per-user fallback - will try to build automatically below\n",
    "]\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Spark session ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f4bd3c-4def-482c-be1a-0b568a496dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from table: workspace.default.test\nData source used -> table:workspace.default.test\nRows: 14900 Columns: 24\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Employee ID</th><th>Age</th><th>Gender</th><th>Years at Company</th><th>Job Role</th><th>Monthly Income</th><th>Work-Life Balance</th><th>Job Satisfaction</th><th>Performance Rating</th><th>Number of Promotions</th><th>Overtime</th><th>Distance from Home</th><th>Education Level</th><th>Marital Status</th><th>Number of Dependents</th><th>Job Level</th><th>Company Size</th><th>Company Tenure</th><th>Remote Work</th><th>Leadership Opportunities</th><th>Innovation Opportunities</th><th>Company Reputation</th><th>Employee Recognition</th><th>Attrition</th></tr></thead><tbody><tr><td>52685</td><td>36</td><td>Male</td><td>13</td><td>Healthcare</td><td>8029</td><td>Excellent</td><td>High</td><td>Average</td><td>1</td><td>Yes</td><td>83</td><td>Master’s Degree</td><td>Married</td><td>1</td><td>Mid</td><td>Large</td><td>22</td><td>No</td><td>No</td><td>No</td><td>Poor</td><td>Medium</td><td>Stayed</td></tr><tr><td>30585</td><td>35</td><td>Male</td><td>7</td><td>Education</td><td>4563</td><td>Good</td><td>High</td><td>Average</td><td>1</td><td>Yes</td><td>55</td><td>Associate Degree</td><td>Single</td><td>4</td><td>Entry</td><td>Medium</td><td>27</td><td>No</td><td>No</td><td>No</td><td>Good</td><td>High</td><td>Left</td></tr><tr><td>54656</td><td>50</td><td>Male</td><td>7</td><td>Education</td><td>5583</td><td>Fair</td><td>High</td><td>Average</td><td>3</td><td>Yes</td><td>14</td><td>Associate Degree</td><td>Divorced</td><td>2</td><td>Senior</td><td>Medium</td><td>76</td><td>No</td><td>No</td><td>Yes</td><td>Good</td><td>Low</td><td>Stayed</td></tr><tr><td>33442</td><td>58</td><td>Male</td><td>44</td><td>Media</td><td>5525</td><td>Fair</td><td>Very High</td><td>High</td><td>0</td><td>Yes</td><td>43</td><td>Master’s Degree</td><td>Single</td><td>4</td><td>Entry</td><td>Medium</td><td>96</td><td>No</td><td>No</td><td>No</td><td>Poor</td><td>Low</td><td>Left</td></tr><tr><td>15667</td><td>39</td><td>Male</td><td>24</td><td>Education</td><td>4604</td><td>Good</td><td>High</td><td>Average</td><td>0</td><td>Yes</td><td>47</td><td>Master’s Degree</td><td>Married</td><td>6</td><td>Mid</td><td>Large</td><td>45</td><td>Yes</td><td>No</td><td>No</td><td>Good</td><td>High</td><td>Stayed</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         52685,
         36,
         "Male",
         13,
         "Healthcare",
         8029,
         "Excellent",
         "High",
         "Average",
         1,
         "Yes",
         83,
         "Master’s Degree",
         "Married",
         1,
         "Mid",
         "Large",
         22,
         "No",
         "No",
         "No",
         "Poor",
         "Medium",
         "Stayed"
        ],
        [
         30585,
         35,
         "Male",
         7,
         "Education",
         4563,
         "Good",
         "High",
         "Average",
         1,
         "Yes",
         55,
         "Associate Degree",
         "Single",
         4,
         "Entry",
         "Medium",
         27,
         "No",
         "No",
         "No",
         "Good",
         "High",
         "Left"
        ],
        [
         54656,
         50,
         "Male",
         7,
         "Education",
         5583,
         "Fair",
         "High",
         "Average",
         3,
         "Yes",
         14,
         "Associate Degree",
         "Divorced",
         2,
         "Senior",
         "Medium",
         76,
         "No",
         "No",
         "Yes",
         "Good",
         "Low",
         "Stayed"
        ],
        [
         33442,
         58,
         "Male",
         44,
         "Media",
         5525,
         "Fair",
         "Very High",
         "High",
         0,
         "Yes",
         43,
         "Master’s Degree",
         "Single",
         4,
         "Entry",
         "Medium",
         96,
         "No",
         "No",
         "No",
         "Poor",
         "Low",
         "Left"
        ],
        [
         15667,
         39,
         "Male",
         24,
         "Education",
         4604,
         "Good",
         "High",
         "Average",
         0,
         "Yes",
         47,
         "Master’s Degree",
         "Married",
         6,
         "Mid",
         "Large",
         45,
         "Yes",
         "No",
         "No",
         "Good",
         "High",
         "Stayed"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Employee ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Years at Company",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Job Role",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Monthly Income",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Work-Life Balance",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Job Satisfaction",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Performance Rating",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Number of Promotions",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Overtime",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Distance from Home",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Education Level",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Marital Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Number of Dependents",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Job Level",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Company Size",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Company Tenure",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Remote Work",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Leadership Opportunities",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Innovation Opportunities",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Company Reputation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Employee Recognition",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Attrition",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 2 — Load dataset into df (try table -> csv candidates)\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "df = None\n",
    "used_source = None\n",
    "\n",
    "# Try table\n",
    "try:\n",
    "    df = spark.table(table_name)\n",
    "    used_source = f\"table:{table_name}\"\n",
    "    print(\"Loaded dataset from table:\", table_name)\n",
    "except Exception:\n",
    "    print(\"Table not found or not accessible, trying CSV candidates...\")\n",
    "\n",
    "# Try CSVs\n",
    "if df is None:\n",
    "    for p in csv_candidates:\n",
    "        try:\n",
    "            tmp = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(p)\n",
    "            # sanity: must have rows\n",
    "            if tmp.count() > 0:\n",
    "                df = tmp\n",
    "                used_source = f\"csv:{p}\"\n",
    "                print(\"Loaded CSV from:\", p)\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if df is None:\n",
    "    raise ValueError(\"Could not find dataset. Upload CSV to DBFS or register table workspace.default.test. Tried candidates: \" + \", \".join(csv_candidates))\n",
    "\n",
    "print(\"Data source used ->\", used_source)\n",
    "print(\"Rows:\", df.count(), \"Columns:\", len(df.columns))\n",
    "display(df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a7a309-5a0e-4079-a0a6-77ef90661dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed columns (original -> cleaned):\n - Employee ID -> employee_id\n - Age -> age\n - Gender -> gender\n - Years at Company -> years_at_company\n - Job Role -> job_role\n - Monthly Income -> monthly_income\n - Work-Life Balance -> work_life_balance\n - Job Satisfaction -> job_satisfaction\n - Performance Rating -> performance_rating\n - Number of Promotions -> number_of_promotions\n - Overtime -> overtime\n - Distance from Home -> distance_from_home\n - Education Level -> education_level\n - Marital Status -> marital_status\n - Number of Dependents -> number_of_dependents\n - Job Level -> job_level\n - Company Size -> company_size\n - Company Tenure -> company_tenure\n - Remote Work -> remote_work\n - Leadership Opportunities -> leadership_opportunities\n - Innovation Opportunities -> innovation_opportunities\n - Company Reputation -> company_reputation\n - Employee Recognition -> employee_recognition\n - Attrition -> attrition\n\nSchema:\nroot\n |-- employee_id: long (nullable = true)\n |-- age: long (nullable = true)\n |-- gender: string (nullable = true)\n |-- years_at_company: long (nullable = true)\n |-- job_role: string (nullable = true)\n |-- monthly_income: long (nullable = true)\n |-- work_life_balance: string (nullable = true)\n |-- job_satisfaction: string (nullable = true)\n |-- performance_rating: string (nullable = true)\n |-- number_of_promotions: long (nullable = true)\n |-- overtime: string (nullable = true)\n |-- distance_from_home: long (nullable = true)\n |-- education_level: string (nullable = true)\n |-- marital_status: string (nullable = true)\n |-- number_of_dependents: long (nullable = true)\n |-- job_level: string (nullable = true)\n |-- company_size: string (nullable = true)\n |-- company_tenure: long (nullable = true)\n |-- remote_work: string (nullable = true)\n |-- leadership_opportunities: string (nullable = true)\n |-- innovation_opportunities: string (nullable = true)\n |-- company_reputation: string (nullable = true)\n |-- employee_recognition: string (nullable = true)\n |-- attrition: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>age</th><th>gender</th><th>years_at_company</th><th>job_role</th><th>monthly_income</th><th>work_life_balance</th><th>job_satisfaction</th><th>performance_rating</th><th>number_of_promotions</th><th>overtime</th><th>distance_from_home</th><th>education_level</th><th>marital_status</th><th>number_of_dependents</th><th>job_level</th><th>company_size</th><th>company_tenure</th><th>remote_work</th><th>leadership_opportunities</th><th>innovation_opportunities</th><th>company_reputation</th><th>employee_recognition</th><th>attrition</th></tr></thead><tbody><tr><td>52685</td><td>36</td><td>Male</td><td>13</td><td>Healthcare</td><td>8029</td><td>Excellent</td><td>High</td><td>Average</td><td>1</td><td>Yes</td><td>83</td><td>Master’s Degree</td><td>Married</td><td>1</td><td>Mid</td><td>Large</td><td>22</td><td>No</td><td>No</td><td>No</td><td>Poor</td><td>Medium</td><td>Stayed</td></tr><tr><td>30585</td><td>35</td><td>Male</td><td>7</td><td>Education</td><td>4563</td><td>Good</td><td>High</td><td>Average</td><td>1</td><td>Yes</td><td>55</td><td>Associate Degree</td><td>Single</td><td>4</td><td>Entry</td><td>Medium</td><td>27</td><td>No</td><td>No</td><td>No</td><td>Good</td><td>High</td><td>Left</td></tr><tr><td>54656</td><td>50</td><td>Male</td><td>7</td><td>Education</td><td>5583</td><td>Fair</td><td>High</td><td>Average</td><td>3</td><td>Yes</td><td>14</td><td>Associate Degree</td><td>Divorced</td><td>2</td><td>Senior</td><td>Medium</td><td>76</td><td>No</td><td>No</td><td>Yes</td><td>Good</td><td>Low</td><td>Stayed</td></tr><tr><td>33442</td><td>58</td><td>Male</td><td>44</td><td>Media</td><td>5525</td><td>Fair</td><td>Very High</td><td>High</td><td>0</td><td>Yes</td><td>43</td><td>Master’s Degree</td><td>Single</td><td>4</td><td>Entry</td><td>Medium</td><td>96</td><td>No</td><td>No</td><td>No</td><td>Poor</td><td>Low</td><td>Left</td></tr><tr><td>15667</td><td>39</td><td>Male</td><td>24</td><td>Education</td><td>4604</td><td>Good</td><td>High</td><td>Average</td><td>0</td><td>Yes</td><td>47</td><td>Master’s Degree</td><td>Married</td><td>6</td><td>Mid</td><td>Large</td><td>45</td><td>Yes</td><td>No</td><td>No</td><td>Good</td><td>High</td><td>Stayed</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         52685,
         36,
         "Male",
         13,
         "Healthcare",
         8029,
         "Excellent",
         "High",
         "Average",
         1,
         "Yes",
         83,
         "Master’s Degree",
         "Married",
         1,
         "Mid",
         "Large",
         22,
         "No",
         "No",
         "No",
         "Poor",
         "Medium",
         "Stayed"
        ],
        [
         30585,
         35,
         "Male",
         7,
         "Education",
         4563,
         "Good",
         "High",
         "Average",
         1,
         "Yes",
         55,
         "Associate Degree",
         "Single",
         4,
         "Entry",
         "Medium",
         27,
         "No",
         "No",
         "No",
         "Good",
         "High",
         "Left"
        ],
        [
         54656,
         50,
         "Male",
         7,
         "Education",
         5583,
         "Fair",
         "High",
         "Average",
         3,
         "Yes",
         14,
         "Associate Degree",
         "Divorced",
         2,
         "Senior",
         "Medium",
         76,
         "No",
         "No",
         "Yes",
         "Good",
         "Low",
         "Stayed"
        ],
        [
         33442,
         58,
         "Male",
         44,
         "Media",
         5525,
         "Fair",
         "Very High",
         "High",
         0,
         "Yes",
         43,
         "Master’s Degree",
         "Single",
         4,
         "Entry",
         "Medium",
         96,
         "No",
         "No",
         "No",
         "Poor",
         "Low",
         "Left"
        ],
        [
         15667,
         39,
         "Male",
         24,
         "Education",
         4604,
         "Good",
         "High",
         "Average",
         0,
         "Yes",
         47,
         "Master’s Degree",
         "Married",
         6,
         "Mid",
         "Large",
         45,
         "Yes",
         "No",
         "No",
         "Good",
         "High",
         "Stayed"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "years_at_company",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "job_role",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "monthly_income",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "work_life_balance",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "job_satisfaction",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "performance_rating",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "number_of_promotions",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "overtime",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "distance_from_home",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "education_level",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "marital_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "number_of_dependents",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "job_level",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company_size",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company_tenure",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "remote_work",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "leadership_opportunities",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "innovation_opportunities",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "company_reputation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_recognition",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "attrition",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 3 — Normalize column names (snake_case) to avoid annoying spaces/issues\n",
    "import re\n",
    "def clean_name(s):\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)  # remove punctuation\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)      # spaces -> underscore\n",
    "    return s.lower()\n",
    "\n",
    "orig_cols = df.columns\n",
    "new_cols = [clean_name(c) for c in orig_cols]\n",
    "df = df.toDF(*new_cols)\n",
    "\n",
    "print(\"Renamed columns (original -> cleaned):\")\n",
    "for o,n in zip(orig_cols, new_cols):\n",
    "    print(f\" - {o} -> {n}\")\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()\n",
    "display(df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21fe2043-d2bf-4fe9-b23f-c9ccd58aa56f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected columns:\n employee id: employee_id\n attrition  : attrition\n hire date  : None\n exit date  : None\n gender     : gender\n department : None\n reputation : company_reputation\n recognition: employee_recognition\n performance: performance_rating\n salary     : None\n"
     ]
    }
   ],
   "source": [
    "# CELL 4 — Detect key columns heuristically\n",
    "cols = df.columns\n",
    "\n",
    "def find_col(phrases):\n",
    "    for p in phrases:\n",
    "        for c in cols:\n",
    "            if p in c:\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "emp_id_col = find_col([\"employee_id\",\"employeeid\",\"emp_id\",\"id\"]) or None\n",
    "attr_col = find_col([\"attrition\",\"left\",\"exited\",\"is_exited\",\"status\"])\n",
    "hire_col = find_col([\"hire_date\",\"hire\",\"joining_date\",\"date_join\"])\n",
    "exit_col = find_col([\"exit_date\",\"resign_date\",\"termination_date\",\"date_of_exit\",\"left_date\"])\n",
    "gender_col = find_col([\"gender\",\"sex\"])\n",
    "dept_col = find_col([\"department\",\"dept\"])\n",
    "reputation_col = find_col([\"company_reputation\",\"reputation\"])\n",
    "recognition_col = find_col([\"employee_recognition\",\"recognition\"])\n",
    "perf_col = find_col([\"performance_rating\",\"performance\"])\n",
    "salary_col = find_col([\"salary\",\"annual_salary\"])\n",
    "\n",
    "print(\"Detected columns:\")\n",
    "print(\" employee id:\", emp_id_col)\n",
    "print(\" attrition  :\", attr_col)\n",
    "print(\" hire date  :\", hire_col)\n",
    "print(\" exit date  :\", exit_col)\n",
    "print(\" gender     :\", gender_col)\n",
    "print(\" department :\", dept_col)\n",
    "print(\" reputation :\", reputation_col)\n",
    "print(\" recognition:\", recognition_col)\n",
    "print(\" performance:\", perf_col)\n",
    "print(\" salary     :\", salary_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e96f5909-5143-41de-94b1-c06f7c7f290d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headcount: 14900\nTotal exited: 7032\nAttrition %: 47.19\n\nValue counts for attrition (if present):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>attrition</th><th>count</th></tr></thead><tbody><tr><td>Stayed</td><td>7868</td></tr><tr><td>Left</td><td>7032</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Stayed",
         7868
        ],
        [
         "Left",
         7032
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "attrition",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView8d595c9\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView8d595c9\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView8d595c9\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView8d595c9) SELECT `attrition`,SUM(`count`) `column_2b2a252d224` FROM q GROUP BY `attrition`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView8d595c9\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "attrition",
             "id": "column_2b2a252d223"
            },
            "y": [
             {
              "column": "count",
              "id": "column_2b2a252d224",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "pie",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_2b2a252d224": {
             "name": "count",
             "type": "pie",
             "yAxis": 0
            }
           },
           "showDataLabels": true,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "fc5bf8ba-5bf4-4a76-9a5d-d84b07eb7073",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 6.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "attrition",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "attrition",
           "type": "column"
          },
          {
           "alias": "column_2b2a252d224",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAttrition by gender (sample):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>gender</th><th>headcount</th><th>exits</th><th>attrition_rate_pct</th></tr></thead><tbody><tr><td>Male</td><td>8087</td><td>3461</td><td>42.8</td></tr><tr><td>Female</td><td>6813</td><td>3571</td><td>52.41</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Male",
         8087,
         3461,
         42.8
        ],
        [
         "Female",
         6813,
         3571,
         52.41
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "headcount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "exits",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "attrition_rate_pct",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 1:\n        # create a temp view\n        if type(__backend_agg_dfs[1]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[1].to_spark().createOrReplaceTempView(\"DatabricksViewbcd451d\")\n        elif type(__backend_agg_dfs[1]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[1], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[1]).createOrReplaceTempView(\"DatabricksViewbcd451d\")\n        else:\n            __backend_agg_dfs[1].createOrReplaceTempView(\"DatabricksViewbcd451d\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewbcd451d) SELECT `gender`,`attrition_rate_pct` FROM q GROUP BY `attrition_rate_pct`,`gender`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewbcd451d\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "attrition_rate_pct",
             "id": "column_2b2a252d227"
            },
            "x": {
             "column": "gender",
             "id": "column_2b2a252d226"
            },
            "y": []
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {},
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "95f9ab8a-5cd3-4442-8377-43d472bca22f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 6.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "gender",
           "type": "column"
          },
          {
           "column": "attrition_rate_pct",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "gender",
           "type": "column"
          },
          {
           "column": "attrition_rate_pct",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 1,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 1:\n        # create a temp view\n        if type(__backend_agg_dfs[1]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[1].to_spark().createOrReplaceTempView(\"DatabricksView7a71b6e\")\n        elif type(__backend_agg_dfs[1]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[1], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[1]).createOrReplaceTempView(\"DatabricksView7a71b6e\")\n        else:\n            __backend_agg_dfs[1].createOrReplaceTempView(\"DatabricksView7a71b6e\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView7a71b6e) SELECT `headcount`,`attrition_rate_pct` FROM q GROUP BY `attrition_rate_pct`,`headcount`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView7a71b6e\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 2",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "attrition_rate_pct",
             "id": "column_2b2a252d234"
            },
            "x": {
             "column": "headcount",
             "id": "column_2b2a252d231"
            },
            "y": []
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {},
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "e89f3a38-c489-4e06-9df1-21f99a3d2a66",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 7.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "headcount",
           "type": "column"
          },
          {
           "column": "attrition_rate_pct",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "headcount",
           "type": "column"
          },
          {
           "column": "attrition_rate_pct",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 1,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 5 — KPIs & breakdowns\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, countDistinct, sum as _sum\n",
    "\n",
    "# Ensure employee id column exists logically\n",
    "if emp_id_col is None:\n",
    "    print(\"No employee_id column detected - headcount will be based on rows.\")\n",
    "    emp_id_col = None\n",
    "\n",
    "# Create _exited_flag if attrition column exists\n",
    "if attr_col:\n",
    "    df = df.withColumn(\"_exited_flag\", when(col(attr_col).rlike(\"(?i)^(yes|y|true|1|left|exited)$\"), 1).otherwise(0))\n",
    "else:\n",
    "    print(\"No attrition column detected. _exited_flag will not be present.\")\n",
    "\n",
    "# HEADCOUNT\n",
    "headcount = df.select(emp_id_col).distinct().count() if emp_id_col else df.count()\n",
    "\n",
    "# TOTAL EXITED & ATTRITION %\n",
    "total_exited = int(df.agg(F.coalesce(_sum(col(\"_exited_flag\")), F.lit(0)).alias(\"total_exited\")).collect()[0][\"total_exited\"]) if \"_exited_flag\" in df.columns else 0\n",
    "attrition_pct = round(total_exited / headcount * 100, 2) if headcount>0 and \"_exited_flag\" in df.columns else None\n",
    "\n",
    "print(\"Headcount:\", headcount)\n",
    "print(\"Total exited:\", total_exited)\n",
    "print(\"Attrition %:\", attrition_pct)\n",
    "\n",
    "print(\"\\nValue counts for attrition (if present):\")\n",
    "if attr_col:\n",
    "    display(df.groupBy(attr_col).count().orderBy(\"count\", ascending=False))\n",
    "\n",
    "# Gender breakdown\n",
    "if gender_col and \"_exited_flag\" in df.columns:\n",
    "    gender_agg = df.groupBy(gender_col).agg(\n",
    "        (countDistinct(emp_id_col) if emp_id_col else F.count(F.lit(1))).alias(\"headcount\"),\n",
    "        _sum(col(\"_exited_flag\")).alias(\"exits\")\n",
    "    ).withColumn(\"attrition_rate_pct\", F.round(col(\"exits\")/col(\"headcount\")*100,2))\n",
    "    print(\"\\nAttrition by gender (sample):\")\n",
    "    display(gender_agg.orderBy(\"headcount\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b463d4-4922-49fb-b2c9-81e67707d517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 6 — Dates & monthly aggregates (optional)\n",
    "from pyspark.sql.functions import to_date, year, month, concat_ws, datediff\n",
    "\n",
    "if hire_col:\n",
    "    df = df.withColumn(\"hire_date_parsed\", to_date(col(hire_col)))\n",
    "if exit_col:\n",
    "    df = df.withColumn(\"exit_date_parsed\", to_date(col(exit_col)))\n",
    "\n",
    "if \"hire_date_parsed\" in df.columns:\n",
    "    df = df.withColumn(\"hire_ym\", concat_ws(\"-\", year(col(\"hire_date_parsed\")), month(col(\"hire_date_parsed\"))))\n",
    "if \"exit_date_parsed\" in df.columns:\n",
    "    df = df.withColumn(\"exit_ym\", concat_ws(\"-\", year(col(\"exit_date_parsed\")), month(col(\"exit_date_parsed\"))))\n",
    "\n",
    "# tenure days\n",
    "if \"hire_date_parsed\" in df.columns:\n",
    "    today = F.current_date()\n",
    "    if \"exit_date_parsed\" in df.columns:\n",
    "        df = df.withColumn(\"tenure_days\", datediff(col(\"exit_date_parsed\"), col(\"hire_date_parsed\")))\n",
    "    else:\n",
    "        df = df.withColumn(\"tenure_days\", datediff(today, col(\"hire_date_parsed\")))\n",
    "\n",
    "monthly_hires = df.groupBy(\"hire_ym\").agg(F.countDistinct(emp_id_col).alias(\"hires\")).orderBy(\"hire_ym\") if \"hire_ym\" in df.columns else None\n",
    "monthly_exits = df.groupBy(\"exit_ym\").agg(F.countDistinct(emp_id_col).alias(\"exits\")).orderBy(\"exit_ym\") if \"exit_ym\" in df.columns else None\n",
    "\n",
    "if monthly_hires is not None:\n",
    "    print(\"Monthly hires sample:\")\n",
    "    display(monthly_hires.limit(20))\n",
    "if monthly_exits is not None:\n",
    "    print(\"Monthly exits sample:\")\n",
    "    display(monthly_exits.limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a08d2c86-c6f0-4f0e-b161-606dae35ca45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAggregate by company_reputation:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>company_reputation</th><th>headcount</th><th>exits</th><th>attrition_rate_pct</th></tr></thead><tbody><tr><td>Good</td><td>7416</td><td>3214</td><td>43.34</td></tr><tr><td>Poor</td><td>3082</td><td>1669</td><td>54.15</td></tr><tr><td>Fair</td><td>2969</td><td>1517</td><td>51.09</td></tr><tr><td>Excellent</td><td>1433</td><td>632</td><td>44.1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Good",
         7416,
         3214,
         43.34
        ],
        [
         "Poor",
         3082,
         1669,
         54.15
        ],
        [
         "Fair",
         2969,
         1517,
         51.09
        ],
        [
         "Excellent",
         1433,
         632,
         44.1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "company_reputation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "headcount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "exits",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "attrition_rate_pct",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBDRUxMIDcg4oCUIEFnZ3JlZ2F0aW9ucyBmb3IgZGFzaGJvYXJkIHRpbGVzICYgZHJpbGxkb3ducwpmcm9tIHB5c3Bhcmsuc3FsLmZ1bmN0aW9ucyBpbXBvcnQgc3VtIGFzIF9zdW0sIHJvdW5kIGFzIHNwYXJrX3JvdW5kCgpncm91cF9jYW5kaWRhdGVzID0gW2RlcHRfY29sLCByZXB1dGF0aW9uX2NvbCwgcmVjb2duaXRpb25fY29sLCBnZW5kZXJfY29sXQpncm91cF9jb2xzID0gW2MgZm9yIGMgaW4gZ3JvdXBfY2FuZGlkYXRlcyBpZiBjIGFuZCBjIGluIGRmLmNvbHVtbnNdCgphZ2dfdGFibGVzID0ge30KZm9yIGcgaW4gZ3JvdXBfY29sczoKICAgIGFnZyA9IGRmLmdyb3VwQnkoZykuYWdnKAogICAgICAgIChGLmNvdW50RGlzdGluY3QoZW1wX2lkX2NvbCkgaWYgZW1wX2lkX2NvbCBlbHNlIEYuY291bnQoRi5saXQoMSkpKS5hbGlhcygiaGVhZGNvdW50IiksCiAgICAgICAgX3N1bSh3aGVuKGNvbCgiX2V4aXRlZF9mbGFnIik9PTEsMSkub3RoZXJ3aXNlKDApKS5hbGlhcygiZXhpdHMiKQogICAgKS53aXRoQ29sdW1uKCJhdHRyaXRpb25fcmF0ZV9wY3QiLCBzcGFya19yb3VuZChjb2woImV4aXRzIikvY29sKCJoZWFkY291bnQiKSoxMDAsMikpLm9yZGVyQnkoImhlYWRjb3VudCIsIGFzY2VuZGluZz1GYWxzZSkKICAgIGFnZ190YWJsZXNbZiJhZ2dfYnlfe2d9Il0gPSBhZ2cKICAgIHByaW50KGYiXG5BZ2dyZWdhdGUgYnkge2d9OiIpCiAgICBkaXNwbGF5KGFnZy5saW1pdCgyMCkpCgojIGluY2x1ZGUgbW9udGhseSBhZ2dyZWdhdGVzIGlmIHByZXNlbnQKaWYgJ21vbnRobHlfaGlyZXMnIGluIGdsb2JhbHMoKSBhbmQgbW9udGhseV9oaXJlcyBpcyBub3QgTm9uZToKICAgIGFnZ190YWJsZXNbIm1vbnRobHlfaGlyZXMiXSA9IG1vbnRobHlfaGlyZXMKaWYgJ21vbnRobHlfZXhpdHMnIGluIGdsb2JhbHMoKSBhbmQgbW9udGhseV9leGl0cyBpcyBub3QgTm9uZToKICAgIGFnZ190YWJsZXNbIm1vbnRobHlfZXhpdHMiXSA9IG1vbnRobHlfZXhpdHMK\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView4f80f68\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView4f80f68\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView4f80f68\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView4f80f68) SELECT `company_reputation`,`attrition_rate_pct` FROM q GROUP BY `attrition_rate_pct`,`company_reputation`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView4f80f68\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "attrition_rate_pct",
             "id": "column_2b2a252d236"
            },
            "x": {
             "column": "company_reputation",
             "id": "column_2b2a252d235"
            },
            "y": []
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {},
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "64fada17-bcfa-4d78-b545-8bdfada6f2a6",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "company_reputation",
           "type": "column"
          },
          {
           "column": "attrition_rate_pct",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "company_reputation",
           "type": "column"
          },
          {
           "column": "attrition_rate_pct",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAggregate by employee_recognition:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_recognition</th><th>headcount</th><th>exits</th><th>attrition_rate_pct</th></tr></thead><tbody><tr><td>Low</td><td>5862</td><td>2800</td><td>47.77</td></tr><tr><td>Medium</td><td>4624</td><td>2180</td><td>47.15</td></tr><tr><td>High</td><td>3706</td><td>1730</td><td>46.68</td></tr><tr><td>Very High</td><td>708</td><td>322</td><td>45.48</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Low",
         5862,
         2800,
         47.77
        ],
        [
         "Medium",
         4624,
         2180,
         47.15
        ],
        [
         "High",
         3706,
         1730,
         46.68
        ],
        [
         "Very High",
         708,
         322,
         45.48
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_recognition",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "headcount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "exits",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "attrition_rate_pct",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBDRUxMIDcg4oCUIEFnZ3JlZ2F0aW9ucyBmb3IgZGFzaGJvYXJkIHRpbGVzICYgZHJpbGxkb3ducwpmcm9tIHB5c3Bhcmsuc3FsLmZ1bmN0aW9ucyBpbXBvcnQgc3VtIGFzIF9zdW0sIHJvdW5kIGFzIHNwYXJrX3JvdW5kCgpncm91cF9jYW5kaWRhdGVzID0gW2RlcHRfY29sLCByZXB1dGF0aW9uX2NvbCwgcmVjb2duaXRpb25fY29sLCBnZW5kZXJfY29sXQpncm91cF9jb2xzID0gW2MgZm9yIGMgaW4gZ3JvdXBfY2FuZGlkYXRlcyBpZiBjIGFuZCBjIGluIGRmLmNvbHVtbnNdCgphZ2dfdGFibGVzID0ge30KZm9yIGcgaW4gZ3JvdXBfY29sczoKICAgIGFnZyA9IGRmLmdyb3VwQnkoZykuYWdnKAogICAgICAgIChGLmNvdW50RGlzdGluY3QoZW1wX2lkX2NvbCkgaWYgZW1wX2lkX2NvbCBlbHNlIEYuY291bnQoRi5saXQoMSkpKS5hbGlhcygiaGVhZGNvdW50IiksCiAgICAgICAgX3N1bSh3aGVuKGNvbCgiX2V4aXRlZF9mbGFnIik9PTEsMSkub3RoZXJ3aXNlKDApKS5hbGlhcygiZXhpdHMiKQogICAgKS53aXRoQ29sdW1uKCJhdHRyaXRpb25fcmF0ZV9wY3QiLCBzcGFya19yb3VuZChjb2woImV4aXRzIikvY29sKCJoZWFkY291bnQiKSoxMDAsMikpLm9yZGVyQnkoImhlYWRjb3VudCIsIGFzY2VuZGluZz1GYWxzZSkKICAgIGFnZ190YWJsZXNbZiJhZ2dfYnlfe2d9Il0gPSBhZ2cKICAgIHByaW50KGYiXG5BZ2dyZWdhdGUgYnkge2d9OiIpCiAgICBkaXNwbGF5KGFnZy5saW1pdCgyMCkpCgojIGluY2x1ZGUgbW9udGhseSBhZ2dyZWdhdGVzIGlmIHByZXNlbnQKaWYgJ21vbnRobHlfaGlyZXMnIGluIGdsb2JhbHMoKSBhbmQgbW9udGhseV9oaXJlcyBpcyBub3QgTm9uZToKICAgIGFnZ190YWJsZXNbIm1vbnRobHlfaGlyZXMiXSA9IG1vbnRobHlfaGlyZXMKaWYgJ21vbnRobHlfZXhpdHMnIGluIGdsb2JhbHMoKSBhbmQgbW9udGhseV9leGl0cyBpcyBub3QgTm9uZToKICAgIGFnZ190YWJsZXNbIm1vbnRobHlfZXhpdHMiXSA9IG1vbnRobHlfZXhpdHMK\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 1:\n        # create a temp view\n        if type(__backend_agg_dfs[1]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[1].to_spark().createOrReplaceTempView(\"DatabricksView1126296\")\n        elif type(__backend_agg_dfs[1]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[1], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[1]).createOrReplaceTempView(\"DatabricksView1126296\")\n        else:\n            __backend_agg_dfs[1].createOrReplaceTempView(\"DatabricksView1126296\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView1126296) SELECT `employee_recognition`,`attrition_rate_pct` FROM q GROUP BY `attrition_rate_pct`,`employee_recognition`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView1126296\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "attrition_rate_pct",
             "id": "column_2b2a252d240"
            },
            "x": {
             "column": "employee_recognition",
             "id": "column_2b2a252d239"
            },
            "y": []
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {},
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "009617bc-d97e-48c4-89c0-26358cca8954",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "employee_recognition",
           "type": "column"
          },
          {
           "column": "attrition_rate_pct",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "employee_recognition",
           "type": "column"
          },
          {
           "column": "attrition_rate_pct",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 1,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAggregate by gender:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>gender</th><th>headcount</th><th>exits</th><th>attrition_rate_pct</th></tr></thead><tbody><tr><td>Male</td><td>8087</td><td>3461</td><td>42.8</td></tr><tr><td>Female</td><td>6813</td><td>3571</td><td>52.41</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Male",
         8087,
         3461,
         42.8
        ],
        [
         "Female",
         6813,
         3571,
         52.41
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "headcount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "exits",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "attrition_rate_pct",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 7 — Aggregations for dashboard tiles & drilldowns\n",
    "from pyspark.sql.functions import sum as _sum, round as spark_round\n",
    "\n",
    "group_candidates = [dept_col, reputation_col, recognition_col, gender_col]\n",
    "group_cols = [c for c in group_candidates if c and c in df.columns]\n",
    "\n",
    "agg_tables = {}\n",
    "for g in group_cols:\n",
    "    agg = df.groupBy(g).agg(\n",
    "        (F.countDistinct(emp_id_col) if emp_id_col else F.count(F.lit(1))).alias(\"headcount\"),\n",
    "        _sum(when(col(\"_exited_flag\")==1,1).otherwise(0)).alias(\"exits\")\n",
    "    ).withColumn(\"attrition_rate_pct\", spark_round(col(\"exits\")/col(\"headcount\")*100,2)).orderBy(\"headcount\", ascending=False)\n",
    "    agg_tables[f\"agg_by_{g}\"] = agg\n",
    "    print(f\"\\nAggregate by {g}:\")\n",
    "    display(agg.limit(20))\n",
    "\n",
    "# include monthly aggregates if present\n",
    "if 'monthly_hires' in globals() and monthly_hires is not None:\n",
    "    agg_tables[\"monthly_hires\"] = monthly_hires\n",
    "if 'monthly_exits' in globals() and monthly_exits is not None:\n",
    "    agg_tables[\"monthly_exits\"] = monthly_exits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07f42eda-e627-42a9-ad27-80df6e8463ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Methodology\n",
    "\n",
    "The methodology for designing the Human Resource Analytics Dashboard followed a structured and theoretically sound process to ensure accuracy, reliability, and actionable insights.\n",
    "\n",
    "### 1. Selection of HR Theme\n",
    "The chosen theme for this project is **Employee Attrition Analysis**, as attrition is one of the most critical challenges in HR management. Understanding the drivers of attrition helps organizations reduce turnover and improve employee retention.\n",
    "\n",
    "### 2. Data Collection and Loading\n",
    "The dataset was imported into Databricks from structured CSV/table format. Databricks was used due to:\n",
    "- scalable compute capability,\n",
    "- built-in Spark support for data processing,\n",
    "- ability to create interactive dashboards.\n",
    "\n",
    "### 3. Data Cleaning and Preparation\n",
    "Data was cleaned and prepared using the following steps:\n",
    "- Standardized column names to avoid errors.\n",
    "- Converted hire and exit dates into proper date formats.\n",
    "- Created an **_exited_flag** variable based on attrition status.\n",
    "- Handled missing values and inconsistent labels.\n",
    "- Created derived fields such as tenure and month-year buckets (hire_ym, exit_ym).\n",
    "\n",
    "### 4. KPI Identification\n",
    "Key Performance Indicators (KPIs) were selected based on HR literature and organizational relevance:\n",
    "- **Headcount**\n",
    "- **Total Exits**\n",
    "- **Attrition Rate (%)**\n",
    "- **Attrition by Department**\n",
    "- **Attrition by Gender**\n",
    "- **Monthly Hires vs Monthly Exits**\n",
    "- **Tenure Analysis**\n",
    "These KPIs directly connect to organizational stability and talent management.\n",
    "\n",
    "### 5. Aggregation and Feature Engineering\n",
    "Group-level aggregations were created to identify high-attrition areas:\n",
    "- Department-wise\n",
    "- Gender-wise\n",
    "- Company reputation-wise\n",
    "- Employee recognition-wise\n",
    "- Monthly trends\n",
    "\n",
    "These aggregations allow drill-down analysis, a core requirement of the dashboard.\n",
    "\n",
    "### 6. Dashboard Creation\n",
    "Databricks display functions and built-in SQL views were used to visualize:\n",
    "- KPIs (cards)\n",
    "- Trends (line charts)\n",
    "- Category breakdowns (bar charts)\n",
    "\n",
    "The design follows the principles of simplicity, clarity, and usability.\n",
    "\n",
    "This methodology ensures the dashboard is reliable, scalable, and grounded in HR analytics best practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c826683-f401-4ad2-a382-0dc618ea27e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Interrelation of Variables and KPI Explanation\n",
    "\n",
    "Understanding how different HR variables influence attrition is essential for interpreting the dashboard.\n",
    "\n",
    "### 1. Department and Attrition\n",
    "Different departments experience varied workloads, team dynamics, and managerial styles.  \n",
    "High attrition in specific departments often signals:\n",
    "- poor leadership,\n",
    "- role mismatch,\n",
    "- workload imbalance.\n",
    "\n",
    "This KPI helps HR direct interventions to the right teams.\n",
    "\n",
    "### 2. Gender and Attrition\n",
    "Gender-based attrition trends highlight diversity and inclusion issues.  \n",
    "For example, higher female attrition may indicate:\n",
    "- lack of flexible policies,\n",
    "- limited leadership opportunities,\n",
    "- workplace cultural challenges.\n",
    "\n",
    "### 3. Tenure and Attrition\n",
    "Tenure is a strong predictor of likelihood to leave:\n",
    "- Employees with low tenure (<1 year) often leave due to job mismatch.\n",
    "- Mid-tenure employees leave due to growth barriers.\n",
    "- Long-tenure employees usually have higher stability.\n",
    "\n",
    "Tenure KPI helps HR identify when interventions should occur.\n",
    "\n",
    "### 4. Company Reputation and Recognition\n",
    "Company reputation and employee recognition strongly influence retention:\n",
    "- Poor reputation reduces engagement.\n",
    "- Low recognition increases dissatisfaction.\n",
    "- High recognition correlates with reduced attrition.\n",
    "\n",
    "These variables help explain why some teams or individuals are at higher exit risk.\n",
    "\n",
    "### 5. Monthly Hires and Monthly Exits\n",
    "Trend analysis helps identify:\n",
    "- Seasonal hiring cycles,\n",
    "- Unusual spikes in exits,\n",
    "- Workforce planning gaps.\n",
    "\n",
    "This KPI supports forecasting and resource planning.\n",
    "\n",
    "### Overall Interrelation\n",
    "All KPIs collectively explain attrition behavior:\n",
    "- **Department** reflects structural issues  \n",
    "- **Gender** reflects cultural/policy influences  \n",
    "- **Tenure** reflects employment lifecycle  \n",
    "- **Reputation & Recognition** reflect engagement  \n",
    "- **Monthly Trends** reflect workforce stability  \n",
    "\n",
    "Together, they create a holistic view of workforce health.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab309f9-fcbb-49a1-bc51-a3379c5d8437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving aggregate agg_by_company_reputation ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/home/spark-cd46a994-df74-4fbb-9083-6b/.ipykernel/1579/command-8133206262356321-1584580540\", line 20, in try_write_to_candidates\n    tbl.write.mode(\"overwrite\").parquet(out_path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 779, in parquet\n    self.format(\"parquet\").save(path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 703, in save\n    _, _, ei = self._spark.client.execute_command(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 1556, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2059, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2035, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2355, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2433, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.UnsupportedOperationException: Public DBFS root is disabled. Access is denied on path: /tmp/hr_dashboard/aggregates/agg_by_company_reputation/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1227)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1224)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1223)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:208)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:249)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3846)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nTraceback (most recent call last):\n  File \"/home/spark-cd46a994-df74-4fbb-9083-6b/.ipykernel/1579/command-8133206262356321-1584580540\", line 20, in try_write_to_candidates\n    tbl.write.mode(\"overwrite\").parquet(out_path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 779, in parquet\n    self.format(\"parquet\").save(path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 703, in save\n    _, _, ei = self._spark.client.execute_command(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 1556, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2059, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2035, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2355, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2433, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.UnsupportedOperationException: Public DBFS root is disabled. Access is denied on path: /tmp/hr_dashboard/aggregates/agg_by_company_reputation/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1227)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1224)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1223)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:208)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:249)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3846)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nTraceback (most recent call last):\n  File \"/home/spark-cd46a994-df74-4fbb-9083-6b/.ipykernel/1579/command-8133206262356321-1584580540\", line 20, in try_write_to_candidates\n    tbl.write.mode(\"overwrite\").parquet(out_path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 779, in parquet\n    self.format(\"parquet\").save(path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 703, in save\n    _, _, ei = self._spark.client.execute_command(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 1556, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2059, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2035, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2355, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2433, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.UnsupportedOperationException: Public DBFS root is disabled. Access is denied on path: /user/akhilamol411_at_gmail_com/hr_dashboard/aggregates/agg_by_company_reputation/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1227)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1224)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1223)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:208)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:249)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3846)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not write agg_by_company_reputation to any candidate path. Please provide a writable DBFS mount path (e.g., dbfs:/mnt/<mountname>/...).\nSaving aggregate agg_by_employee_recognition ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/home/spark-cd46a994-df74-4fbb-9083-6b/.ipykernel/1579/command-8133206262356321-1584580540\", line 20, in try_write_to_candidates\n    tbl.write.mode(\"overwrite\").parquet(out_path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 779, in parquet\n    self.format(\"parquet\").save(path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 703, in save\n    _, _, ei = self._spark.client.execute_command(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 1556, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2059, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2035, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2355, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2433, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.UnsupportedOperationException: Public DBFS root is disabled. Access is denied on path: /tmp/hr_dashboard/aggregates/agg_by_employee_recognition/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1227)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1224)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1223)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:208)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:249)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3846)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nTraceback (most recent call last):\n  File \"/home/spark-cd46a994-df74-4fbb-9083-6b/.ipykernel/1579/command-8133206262356321-1584580540\", line 20, in try_write_to_candidates\n    tbl.write.mode(\"overwrite\").parquet(out_path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 779, in parquet\n    self.format(\"parquet\").save(path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 703, in save\n    _, _, ei = self._spark.client.execute_command(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 1556, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2059, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2035, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2355, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2433, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.UnsupportedOperationException: Public DBFS root is disabled. Access is denied on path: /tmp/hr_dashboard/aggregates/agg_by_employee_recognition/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1227)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1224)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1223)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:208)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:249)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3846)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not write agg_by_employee_recognition to any candidate path. Please provide a writable DBFS mount path (e.g., dbfs:/mnt/<mountname>/...).\nSaving aggregate agg_by_gender ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/home/spark-cd46a994-df74-4fbb-9083-6b/.ipykernel/1579/command-8133206262356321-1584580540\", line 20, in try_write_to_candidates\n    tbl.write.mode(\"overwrite\").parquet(out_path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 779, in parquet\n    self.format(\"parquet\").save(path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 703, in save\n    _, _, ei = self._spark.client.execute_command(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 1556, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2059, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2035, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2355, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2433, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.UnsupportedOperationException: Public DBFS root is disabled. Access is denied on path: /user/akhilamol411_at_gmail_com/hr_dashboard/aggregates/agg_by_employee_recognition/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1227)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1224)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1223)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:208)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:249)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3846)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nTraceback (most recent call last):\n  File \"/home/spark-cd46a994-df74-4fbb-9083-6b/.ipykernel/1579/command-8133206262356321-1584580540\", line 20, in try_write_to_candidates\n    tbl.write.mode(\"overwrite\").parquet(out_path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 779, in parquet\n    self.format(\"parquet\").save(path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 703, in save\n    _, _, ei = self._spark.client.execute_command(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 1556, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2059, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2035, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2355, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2433, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.UnsupportedOperationException: Public DBFS root is disabled. Access is denied on path: /tmp/hr_dashboard/aggregates/agg_by_gender/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1227)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1224)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1223)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:208)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:249)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3846)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nTraceback (most recent call last):\n  File \"/home/spark-cd46a994-df74-4fbb-9083-6b/.ipykernel/1579/command-8133206262356321-1584580540\", line 20, in try_write_to_candidates\n    tbl.write.mode(\"overwrite\").parquet(out_path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 779, in parquet\n    self.format(\"parquet\").save(path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 703, in save\n    _, _, ei = self._spark.client.execute_command(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 1556, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2059, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2035, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2355, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2433, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.UnsupportedOperationException: Public DBFS root is disabled. Access is denied on path: /tmp/hr_dashboard/aggregates/agg_by_gender/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1227)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1224)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1223)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:208)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:249)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3846)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not write agg_by_gender to any candidate path. Please provide a writable DBFS mount path (e.g., dbfs:/mnt/<mountname>/...).\n\nSaved paths summary:\n\nYou can query these in SQL using global_temp.<view_name>, e.g.:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/home/spark-cd46a994-df74-4fbb-9083-6b/.ipykernel/1579/command-8133206262356321-1584580540\", line 20, in try_write_to_candidates\n    tbl.write.mode(\"overwrite\").parquet(out_path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 779, in parquet\n    self.format(\"parquet\").save(path)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py\", line 703, in save\n    _, _, ei = self._spark.client.execute_command(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 1556, in execute_command\n    data, _, metrics, observed_metrics, properties = self._execute_and_fetch(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2059, in _execute_and_fetch\n    for response in self._execute_and_fetch_as_iterator(\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2035, in _execute_and_fetch_as_iterator\n    self._handle_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2355, in _handle_error\n    self._handle_rpc_error(error)\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 2433, in _handle_rpc_error\n    raise convert_exception(\npyspark.errors.exceptions.connect.UnsupportedOperationException: Public DBFS root is disabled. Access is denied on path: /user/akhilamol411_at_gmail_com/hr_dashboard/aggregates/agg_by_gender/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1227)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1224)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:766)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1223)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:208)\n\tat com.databricks.sql.io.LokiFileSystem.$anonfun$getFileStatusNoCache$3(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.tryWithNativeIO(LokiFileSystem.scala:249)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatusNoCache(LokiFileSystem.scala:268)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:273)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3846)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\n"
     ]
    }
   ],
   "source": [
    "# CELL 8 — Save aggregates to a safe location and register as global_temp views\n",
    "import traceback, os\n",
    "\n",
    "# Build safe per-user output root (try current_user -> dbfs:/user/<user>)\n",
    "try:\n",
    "    user = spark.sql(\"select current_user()\").collect()[0][0]\n",
    "    safe_user = user.replace(\":\", \"_\").replace(\"@\", \"_at_\").replace(\".\", \"_\")\n",
    "    output_base_candidates.append(f\"dbfs:/user/{safe_user}/hr_dashboard/aggregates\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Always keep dbfs:/tmp as candidate\n",
    "output_base_candidates.insert(0, \"dbfs:/tmp/hr_dashboard/aggregates\")\n",
    "\n",
    "def try_write_to_candidates(tbl, name):\n",
    "    for root in output_base_candidates:\n",
    "        out_path = root.rstrip(\"/\") + f\"/{name}\"\n",
    "        try:\n",
    "            # attempt write\n",
    "            tbl.write.mode(\"overwrite\").parquet(out_path)\n",
    "            return out_path\n",
    "        except Exception:\n",
    "            # try next candidate\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "saved_paths = {}\n",
    "for name, tbl in agg_tables.items():\n",
    "    print(f\"Saving aggregate {name} ...\")\n",
    "    outp = try_write_to_candidates(tbl, name)\n",
    "    if outp:\n",
    "        saved_paths[name] = outp\n",
    "        print(f\"Saved {name} -> {outp}\")\n",
    "        # try registering global_temp view\n",
    "        try:\n",
    "            df_tmp = spark.read.parquet(outp)\n",
    "            # view name safe\n",
    "            view_name = name\n",
    "            df_tmp.createOrReplaceGlobalTempView(view_name)\n",
    "            print(f\"Registered global_temp.{view_name} (rows: {df_tmp.count()}, cols: {len(df_tmp.columns)})\")\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(f\"Could not write {name} to any candidate path. Please provide a writable DBFS mount path (e.g., dbfs:/mnt/<mountname>/...).\")\n",
    "\n",
    "print(\"\\nSaved paths summary:\")\n",
    "for k,v in saved_paths.items():\n",
    "    print(\" -\", k, \"->\", v)\n",
    "\n",
    "print(\"\\nYou can query these in SQL using global_temp.<view_name>, e.g.:\")\n",
    "for k in saved_paths.keys():\n",
    "    print(f\"SELECT * FROM global_temp.{k} LIMIT 100;\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edbb892b-5f14-411d-bbde-cbc0b1f4aa65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global temp views available:\n[]\n"
     ]
    }
   ],
   "source": [
    "# CELL 9 — Verify global_temp views and show sample rows\n",
    "print(\"Global temp views available:\")\n",
    "print(spark.catalog.listTables(\"global_temp\"))\n",
    "\n",
    "for view in spark.catalog.listTables(\"global_temp\"):\n",
    "    print(\"View:\", view.name, \"Temporary:\", view.isTemporary)\n",
    "    try:\n",
    "        print(\"Sample rows:\")\n",
    "        display(spark.sql(f\"SELECT * FROM global_temp.{view.name} LIMIT 5\"))\n",
    "    except Exception as e:\n",
    "        print(\"Could not query view:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70f8336d-06de-4870-8bc5-a9e90f8dd6ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Cost, ROI Formula, and Economic Justification**\n",
    "\n",
    "### **1. Cost Assumptions**\n",
    "To ensure the dashboard is economically viable, the total cost of creating and maintaining the HR Analytics Dashboard includes:\n",
    "\n",
    "#### **a. Compute Cost (DBU Hours)**\n",
    "- Databricks notebooks run on a compute cluster measured in **DBU-hours**.\n",
    "- For academic or community usage, cost is negligible or zero.\n",
    "- For enterprise environments:\n",
    "  - Example: 0.15 DBU/hour (Small Job Cluster)\n",
    "  - Estimated usage for this project: **3 hours**\n",
    "  - **Compute Cost ≈ 3 hours × 0.15 DBU × ₹x/DBU**  \n",
    "  - Since student usage is free → **₹0 Actual Cost**\n",
    "\n",
    "#### **b. Storage Cost**\n",
    "- Dashboard output (parquet tables) stored in DBFS.\n",
    "- Storage requirement: ~10–20 MB  \n",
    "- Cloud storage cost ~₹2–₹5 per month (negligible).\n",
    "\n",
    "#### **c. Human Effort (One-time Development Time)**\n",
    "- Time spent designing the dashboard, cleaning data, and building KPIs.\n",
    "- Estimated: **6–8 hours**\n",
    "- This is a one-time investment.\n",
    "\n",
    "### **2. ROI Formula**\n",
    "ROI for an HR dashboard is measured in terms of improved decision-making and reduced HR losses (attrition, overtime, inefficiency), not monetary output alone.\n",
    "\n",
    "A standard ROI formula:\n",
    "\n",
    "\\[\n",
    "\\textbf{ROI (\\%)} = \\frac{\\text{Value Gained from Dashboard} - \\text{Cost of Dashboard}}{\\text{Cost of Dashboard}} \\times 100\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "\n",
    "#### **Value Gained includes:**\n",
    "- Reduction in attrition through early identification of risk.\n",
    "- Savings in recruitment and training costs.\n",
    "- Improved workforce planning.\n",
    "- Faster HR decision-making.\n",
    "- Increased employee satisfaction and productivity.\n",
    "\n",
    "#### **Example Scenario (Illustrative)**\n",
    "- Dashboard helps HR reduce attrition by **1 employee per month**.\n",
    "- Average replacement cost per employee = **₹25,000–₹40,000**.\n",
    "- Annual savings ≈ **₹3–4.8 lakh**.\n",
    "- Dashboard cost ≈ **₹0–500** (student/lab cost is zero).\n",
    "\n",
    "\\[\n",
    "\\textbf{ROI} = \\frac{3{,}00{,}000 - 500}{500} \\times 100 = \\textbf{59,900\\%}\n",
    "\\]\n",
    "\n",
    "This demonstrates that the HR Dashboard has **extremely high ROI** due to minimal cost and significant strategic value.\n",
    "\n",
    "### **3. Conclusion**\n",
    "- The dashboard is **economically viable**, **low-cost**, and **high-impact**.\n",
    "- It improves HR transparency, enhances data-driven decision-making, and reduces attrition-related losses.\n",
    "- The ROI is overwhelmingly positive, fulfilling the project requirement of using a process with a **successful and scalable track record**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431f9845-582c-4e7b-8a17-29b19f4fcd9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion and Recommendations\n",
    "\n",
    "### 1. Summary of Findings\n",
    "The HR Dashboard provides a comprehensive understanding of workforce dynamics. Key findings include:\n",
    "- Departments with high exits indicate management or workload issues.\n",
    "- Gender disparities in attrition highlight inclusion or policy gaps.\n",
    "- Tenure analysis reveals when employees are most likely to leave.\n",
    "- Monthly hire/exit trends show staffing stability.\n",
    "- Low recognition or company reputation correlates strongly with higher attrition.\n",
    "\n",
    "### 2. Managerial Recommendations\n",
    "Based on insights, HR should implement:\n",
    "- **Targeted retention strategies** for departments with high turnover.\n",
    "- **Recognition programs** to improve engagement.\n",
    "- **Structured onboarding** for new employees (as short-tenure attrition is often high).\n",
    "- **Career development pathways** to retain mid-tenure employees.\n",
    "- **Diversity-supportive policies** for gender balance.\n",
    "- **Leadership training** where department-level attrition is high.\n",
    "\n",
    "### 3. Strategic Benefits\n",
    "The dashboard enables:\n",
    "- Faster decision-making,\n",
    "- Evidence-based HR planning,\n",
    "- Identification of root causes,\n",
    "- Improved employee experience,\n",
    "- Reduced recruitment and training costs.\n",
    "\n",
    "### 4. Future Enhancements\n",
    "- Predictive attrition modeling (logistic regression).\n",
    "- Real-time integration with HRIS systems.\n",
    "- More KPIs such as overtime, performance ratings, and training hours.\n",
    "\n",
    "The overall conclusion is that the dashboard offers significant value by improving visibility into workforce trends, supporting strategic HR decisions, and ultimately promoting organizational stability.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": null,
       "elementNUID": "fc5bf8ba-5bf4-4a76-9a5d-d84b07eb7073",
       "elementType": "command",
       "guid": "28d2becc-0510-4400-b881-85d8b779030f",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 12,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "95f9ab8a-5cd3-4442-8377-43d472bca22f",
       "elementType": "command",
       "guid": "43e91b0d-f111-4fb8-bcb7-ecb830c6ae96",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 12,
        "y": 12,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 1,
       "elementNUID": "a08d2c86-c6f0-4f0e-b161-606dae35ca45",
       "elementType": "command",
       "guid": "4751f707-df1b-498b-bdb5-4c15a1848a66",
       "options": null,
       "position": {
        "height": 5,
        "width": 12,
        "x": 0,
        "y": 4,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "56f4bd3c-4def-482c-be1a-0b568a496dd5",
       "elementType": "command",
       "guid": "543ee687-e69f-43c1-affb-dee9a6ddedfe",
       "options": null,
       "position": {
        "height": 4,
        "width": 12,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "32a7a309-5a0e-4079-a0a6-77ef90661dd5",
       "elementType": "command",
       "guid": "5e72dc66-a294-421f-9adb-cbc8fff15a8b",
       "options": null,
       "position": {
        "height": 4,
        "width": 11,
        "x": 12,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "64fada17-bcfa-4d78-b545-8bdfada6f2a6",
       "elementType": "command",
       "guid": "665b8ebc-baa3-4a27-a6b9-32e9f88c2f69",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 18,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "e96f5909-5143-41de-94b1-c06f7c7f290d",
       "elementType": "command",
       "guid": "6d18a769-1163-41db-bdbf-df1be49fcf55",
       "options": null,
       "position": {
        "height": 4,
        "width": 12,
        "x": 12,
        "y": 4,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "e89f3a38-c489-4e06-9df1-21f99a3d2a66",
       "elementType": "command",
       "guid": "78973d7f-3ac8-4504-9ddc-39b9745e0b37",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 12,
        "y": 18,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "009617bc-d97e-48c4-89c0-26358cca8954",
       "elementType": "command",
       "guid": "9dec0e2f-f9df-4196-99fc-3309b7bc8577",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 24,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 2,
       "elementNUID": "a08d2c86-c6f0-4f0e-b161-606dae35ca45",
       "elementType": "command",
       "guid": "a31ce333-4fde-4204-a9e0-1703bba3ad44",
       "options": null,
       "position": {
        "height": 3,
        "width": 12,
        "x": 0,
        "y": 9,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "a08d2c86-c6f0-4f0e-b161-606dae35ca45",
       "elementType": "command",
       "guid": "f37ee9e9-513e-4d81-a136-63efad151979",
       "options": null,
       "position": {
        "height": 4,
        "width": 11,
        "x": 12,
        "y": 8,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "3e8fed65-271c-4f8b-8126-46d9fb1aad65",
     "origId": 8914315745498593,
     "title": "Dashboard -HRA",
     "version": "DashboardViewV1",
     "width": 1024
    },
    {
     "elements": [
      {
       "dashboardResultIndex": 2,
       "elementNUID": "a08d2c86-c6f0-4f0e-b161-606dae35ca45",
       "elementType": "command",
       "guid": "025f761c-d0fd-42e3-b99d-9cb3179b06a7",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "e89f3a38-c489-4e06-9df1-21f99a3d2a66",
       "elementType": "command",
       "guid": "0648687c-b141-411e-af19-4b8abd8bec10",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 6,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "70f8336d-06de-4870-8bc5-a9e90f8dd6ef",
       "elementType": "command",
       "guid": "1893cca7-baad-4b4e-86a7-0d2ba2bc88d0",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 12,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "95f9ab8a-5cd3-4442-8377-43d472bca22f",
       "elementType": "command",
       "guid": "2303ccaf-53f3-4843-a93f-54f74885082c",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 18,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "e96f5909-5143-41de-94b1-c06f7c7f290d",
       "elementType": "command",
       "guid": "482826b2-f603-4b13-b184-a96604f2d778",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 24,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "431f9845-582c-4e7b-8a17-29b19f4fcd9e",
       "elementType": "command",
       "guid": "626b128f-7337-42d4-9bb4-e7a757adbda4",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 30,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 1,
       "elementNUID": "a08d2c86-c6f0-4f0e-b161-606dae35ca45",
       "elementType": "command",
       "guid": "721e8e9a-65ff-4d62-b91f-0d7b284f99a1",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 36,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "07f42eda-e627-42a9-ad27-80df6e8463ca",
       "elementType": "command",
       "guid": "731779e0-9e60-496d-8b9a-482aee5d2e19",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 42,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "0c826683-f401-4ad2-a382-0dc618ea27e9",
       "elementType": "command",
       "guid": "7c4611e6-ef8b-413c-825f-552634d93293",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 48,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "a08d2c86-c6f0-4f0e-b161-606dae35ca45",
       "elementType": "command",
       "guid": "86cbf0ac-9489-45fc-bec1-5fddbc5f55da",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 54,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 1,
       "elementNUID": "e96f5909-5143-41de-94b1-c06f7c7f290d",
       "elementType": "command",
       "guid": "ac6c8b21-5580-4132-a478-6bd923221572",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 60,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "64fada17-bcfa-4d78-b545-8bdfada6f2a6",
       "elementType": "command",
       "guid": "c8f102f5-d308-44ad-88e3-74489bb6dff0",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 66,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "32a7a309-5a0e-4079-a0a6-77ef90661dd5",
       "elementType": "command",
       "guid": "ca27006e-cff1-4515-9107-b80ebbe37d54",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 72,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "56f4bd3c-4def-482c-be1a-0b568a496dd5",
       "elementType": "command",
       "guid": "dee2d4d0-1b93-4852-9e6e-3a213ae35290",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 78,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "009617bc-d97e-48c4-89c0-26358cca8954",
       "elementType": "command",
       "guid": "e0e41382-4292-4d98-b226-ac9cee7c5b8c",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 84,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "fc5bf8ba-5bf4-4a76-9a5d-d84b07eb7073",
       "elementType": "command",
       "guid": "f8de454a-8d14-4aec-8cd6-762a55362b0d",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 90,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "e101f280-e62c-44ed-bf0f-ee6695ffd534",
     "origId": 4727819796911932,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HRA-INTERNAL-2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}